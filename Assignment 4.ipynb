{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e908d95-f89d-492a-91a0-3e55cac27270",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is the purpose of the General Linear Model (GLM)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4cf29a-4114-40f9-8fd8-e81919d3ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "The General Linear Model (GLM) is a statistical framework that allows us to model the relationship between a dependent variable and one or\n",
    "more independent variables. It is a generalization of multiple linear regression to the case of more than one dependent variable. \n",
    "GLM can be used to model continuous, binary, count, and other types of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ff306f-cce6-44e6-b72a-d54cbe82a584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc315cd0-292c-4844-a836-19d09e14eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What are the key assumptions of the General Linear Model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bcb81c-9229-4fd2-93fa-4d70575c9372",
   "metadata": {},
   "outputs": [],
   "source": [
    "The key assumptions of the General Linear Model are:\n",
    "    \n",
    "- Linearity: The relationship between the dependent variable and the independent variables is linear.\n",
    "- Independence: The observations are independent of each other.\n",
    "- Homoscedasticity: The variance of the errors is constant across all levels of the independent variables.\n",
    "- Normality: The errors are normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdad447-3868-4980-9472-de77ae39671b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaa49c9-0f1f-4b7f-8a57-e9d8c084b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. How do you interpret the coefficients in a GLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dadd52b-550f-47b3-902e-24e23c9b589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "In a GLM, the coefficients represent the change in the log-odds (in case of binary data) or log-counts (in case of count data) for a \n",
    "one-unit increase in the corresponding independent variable, holding all other variables constant. A positive coefficient indicates that \n",
    "an increase in the corresponding independent variable is associated with an increase in the dependent variable, while a negative coefficient \n",
    "indicates that an increase in the corresponding independent variable is associated with a decrease in the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f283b245-7e65-4247-8f96-b4ece1bce2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539396a4-215e-4372-b8da-027997a74e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. What is the difference between a univariate and multivariate GLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b158dc8-8aa8-4d8f-9010-113460db7488",
   "metadata": {},
   "outputs": [],
   "source": [
    "A univariate GLM has only one dependent variable, while a multivariate GLM has more than one dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dd8583-0d4d-4992-945e-c16ba4207e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848d1f28-1846-4a5a-93b9-02172ce29d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Explain the concept of interaction effects in a GLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681d7e4f-b244-4ddf-9d06-51ab13890e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "Interaction effects occur when the effect of one independent variable on the dependent variable depends on the level of another independent\n",
    "variable. For example, if we are modeling the relationship between income and education level on job satisfaction, \n",
    "we might find that the effect of income on job satisfaction depends on education level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6251d905-14d6-4a8c-bf21-3d4c48d83fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec4cb03-050e-444e-9bc0-f66a909884f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. How do you handle categorical predictors in a GLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f01aa7f-7a01-4017-8af4-e33d0c2f97be",
   "metadata": {},
   "outputs": [],
   "source": [
    "In a GLM, categorical predictors can be handled by creating dummy variables for each category and including them as independent variables\n",
    "in the model. For example, if we have a categorical predictor with three categories (A, B, and C), we can create two dummy variables\n",
    "(D1 and D2) such that D1=1 if category A is present and 0 otherwise, D2=1 if category B is present and 0 otherwise, and include them along\n",
    "with category C as independent variables in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8b162f-efe1-4a19-ad17-8ab42822ee2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e9b15b-bdfb-48f2-b155-4be8f5dfcec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. What is the purpose of the design matrix in a GLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba441f94-56d5-42d5-886a-ae98f1e01d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "The design matrix in a GLM is a matrix that contains all of the independent variables used in the model. \n",
    "Each row represents an observation, and each column represents an independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542609b4-3ebc-4cbf-bae2-e92e8f39aa39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9892677e-d74b-4274-b045-bd5793f1a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. How do you test the significance of predictors in a GLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66daf33-6d63-4307-ad2b-4c67ceb000b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "The significance of predictors in a GLM can be tested using hypothesis tests based on their corresponding t-statistics or p-values.\n",
    "A predictor is considered significant if its p-value is less than some pre-specified threshold (e.g., 0.05)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deaff10-52ac-4e9b-b632-650e88040f64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805d8007-7b37-464d-a9c7-6a9361ff4640",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. What is the difference between Type I, Type II, and Type III sums of squares in a GLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5208192-efa9-4833-a1af-b1575367abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Type I, Type II, and Type III sums of squares are different methods for partitioning the total sum of squares into components associated \n",
    "with each independent variable in a GLM. Type I sums of squares measure the unique contribution of each independent variable to the model\n",
    "when entered first into the model. Type II sums of squares measure the unique contribution of each independent variable to the model after\n",
    "accounting for all other variables already in the model. Type III sums of squares measure the unique contribution of each independent variable \n",
    "to the model after accounting for all other variables already in the model as well as any interaction terms involving that variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86350fda-c5fe-4fed-b0b1-6844421ea130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb271e9-1097-4e81-940d-6f90620916bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. Explain the concept of deviance in a GLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033618d6-5475-42b5-8eb2-0d9f54a2bea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Deviance is a measure of goodness-of-fit for a generalized linear model. It measures how well our fitted model fits our data compared to \n",
    "a perfect model that perfectly fits our data (known as saturated model). \n",
    "Deviance can be thought as how much variation in our data does our fitted model account for. \n",
    "Lower deviance indicates better fit while higher deviance indicates worse fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa78ba8-191c-4ad0-aedf-206fb026a922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b40794f1-20f8-43cc-8d38-a1a7d2c49519",
   "metadata": {},
   "source": [
    "### Regression:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f67e76-52b2-4e86-8655-c517a65f7cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "11. What is regression analysis and what is its purpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d247ea4-79d6-4d69-be72-f6afe7606af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regression analysis is a statistical method used to model the relationship between a dependent variable and one or more independent variables.\n",
    "The purpose of regression analysis is to predict the value of the dependent variable based on the values of the independent variables.\n",
    "It can be used to model continuous, binary, count, and other types of data. Regression analysis can help with forecasting, decision making,\n",
    "and explaining past occurrences in various fields¹³. The most common models are simple linear and multiple linear.\n",
    "Simple linear regression is used when there is a linear relationship between two variables, while multiple linear regression is used when \n",
    "there are two or more independent variables². The R-squared value in regression measures how well the regression line fits the data, with\n",
    "higher values indicating a better fit³. Correlation measures the strength of a linear relationship between two variables, while regression\n",
    "models the relationship between a dependent variable and one or more independent variables⁴. The coefficients in regression represent the\n",
    "change in the dependent variable for each 1 unit change in an independent variable when you hold all of the other independent variables\n",
    "constant, while the intercept represents the mean value of the dependent variable when all of the predictor variables are equal to zero.\n",
    "Outliers can be handled in regression analysis by removing them, transforming the dependent variable, or using robust methods of regression.\n",
    "Polynomial regression is a form of regression analysis in which the relationship between the independent variable and dependent variable is\n",
    "modeled as an nth degree polynomial in x. It is used when there is a nonlinear relationship between two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68434c26-9647-4f16-a689-76b22de3798a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f4770e-62f3-4dc6-8b48-e9ec8b7868fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "12. What is the difference between simple linear regression and multiple linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2790d3-7035-4767-806a-f2c1a0b0d5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Simple linear regression is a statistical method used to model the relationship between two variables, where one variable is the dependent\n",
    "variable and the other variable is the independent variable. The purpose of simple linear regression is to predict the value of the dependent\n",
    "variable based on the value of the independent variable. Simple linear regression can be used to model continuous, binary, count, and other\n",
    "types of data. Multiple linear regression is a statistical method used to model the relationship between a dependent variable and two or more\n",
    "independent variables. The purpose of multiple linear regression is to predict the value of the dependent variable based on the values of the\n",
    "independent variables. Multiple linear regression can be used to model continuous, binary, count, and other types of data.\n",
    "The main differences between simple linear regression and multiple linear regression are:\n",
    "    \n",
    "- Simple linear regression has only one x and one y variable. Multiple linear regression has one y and two or more x variables.\n",
    "- Simple linear regression occurs in 2 dimensions, while multiple linear regression can occur in an infinite number of dimensions.\n",
    "- Simple linear regression is used when you have only one predictor, or X variable, predicting the response or Y variable. Multiple regression\n",
    "  is used when you have multiple X predictors that all contribute to predicting Y.\n",
    "- In simple linear regression, a criterion variable is predicted from one predictor variable. In multiple regression, the criterion is\n",
    "  predicted by two or more variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9113c46-37f5-454a-b082-409c48997efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031ce9e5-f81d-4283-9149-156ec2494155",
   "metadata": {},
   "outputs": [],
   "source": [
    "13. How do you interpret the R-squared value in regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eae2f0-d0a8-4569-92e1-93b1ea671778",
   "metadata": {},
   "outputs": [],
   "source": [
    "The R-squared value in regression measures how well the regression line fits the data, with higher values indicating a better fit.\n",
    "It is also called the coefficient of determination, or the coefficient of multiple determination for multiple regression.\n",
    "R-squared is the percentage of the dependent variable variation that a linear model explains.\n",
    "R-squared is always between 0 and 100%: 0% represents a model that does not explain any of the variation in the response variable around its\n",
    "mean.\n",
    "The mean of the dependent variable predicts the dependent variable as well as the regression model. 100% represents a model that explains all\n",
    "the variation in the response variable around its mean. Usually, the larger the R-squared value, the better the regression model fits your\n",
    "observations123. However, it is important to note that a high R-squared value does not necessarily mean that your model is good or that it\n",
    "will make accurate predictions.\n",
    "A low R-squared value does not necessarily mean that your model is bad or that it will make inaccurate predictions.\n",
    "It is always important to evaluate your model using other metrics and to use your domain knowledge to interpret your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7463684-4513-4081-8295-c296c2495967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bebd136-fa18-4962-9003-80f740c99837",
   "metadata": {},
   "outputs": [],
   "source": [
    "14. What is the difference between correlation and regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9bfcef-aa99-4c18-8b68-0d26ca3fe5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Correlation and regression are two statistical methods used to measure the relationship between two variables.\n",
    "Correlation measures the strength and direction of a linear relationship between two variables, while regression models the relationship\n",
    "between a dependent variable and one or more independent variables.\n",
    "Correlation coefficients range from -1 to 1, with -1 indicating a perfect negative correlation, 0 indicating no correlation, and 1 indicating\n",
    "a perfect positive correlation.\n",
    "Regression coefficients represent the change in the dependent variable for each 1 unit change in an independent variable when you hold all of\n",
    "the other independent variables constant.\n",
    "Regression can be used to predict the value of the dependent variable based on the values of the independent variables, while correlation \n",
    "cannot be used for prediction. In other words, regression is a predictive model, while correlation is not.\n",
    "Correlation does not imply causation, while regression can be used to test causal hypotheses ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f51ee8-c7b2-40cb-8751-b2f3e35515d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d287c4-0652-4321-895e-63cdfeaf6535",
   "metadata": {},
   "outputs": [],
   "source": [
    "15. What is the difference between the coefficients and the intercept in regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ebb7d3-1203-400b-acd2-2ff8f0084503",
   "metadata": {},
   "outputs": [],
   "source": [
    "In regression analysis, the coefficients and the intercept are two important parameters that help us understand the relationship between the\n",
    "dependent variable and the independent variables. \n",
    "The coefficients represent the change in the dependent variable for each one-unit increase in the corresponding independent variable,\n",
    "holding all other variables constant. \n",
    "The intercept represents the value of the dependent variable when all independent variables are equal to zero.\n",
    "In other words, it is the expected value of the dependent variable when all independent variables are zero.\n",
    "The intercept is also called the constant term or bias term.\n",
    "The coefficients and the intercept are estimated from the data using a statistical method called least squares regression.\n",
    "The coefficients and the intercept can be used to make predictions about the dependent variable based on the values of the independent \n",
    "variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211f97da-49c6-4334-b80d-73159018e893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed6cb65-6aec-41a5-baab-20ab070f1a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "16. How do you handle outliers in regression analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0166d60-7e18-431c-a7be-56fc5254e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are several techniques to handle outliers in regression analysis, such as:\n",
    "- Trimming/Remove the outliers\n",
    "- Quantile based flooring and capping\n",
    "- Mean/Median imputation\n",
    "- Transformation\n",
    "- Using a different model\n",
    "- Using robust methods of regression\n",
    "- Excluding outliers that are bad data, such as typos¹.\n",
    "\n",
    "One of the most common ways to handle outliers is to remove them from the dataset. However, this approach can be problematic if the outliers\n",
    "are not due to measurement error or data entry errors. Another approach is to use robust regression methods that are less sensitive to\n",
    "outliers, such as M-estimation or S-estimation³. Another way to handle outliers is to transform the data using a logarithmic or power \n",
    "transformation. This can help reduce the impact of extreme values on the regression model.\n",
    "\n",
    "It is important to note that there is no one-size-fits-all solution for handling outliers in regression analysis. \n",
    "The best approach depends on the nature of the data and the research question being addressed.\n",
    "It is always a good idea to consult with a statistician or data analyst before deciding on an approach for handling outliers in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3ce69a-0356-48a6-8ba5-8c9ca83b0952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e661e32-9e47-4da0-9b60-e1285876e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "17. What is the difference between ridge regression and ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3db84b-8280-4a13-8c57-c3828f378a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge regression and ordinary least squares (OLS) regression are both methods used to estimate the parameters of a linear regression model.\n",
    "However, they differ in the way they estimate the coefficients of the model.\n",
    "OLS regression estimates the coefficients by minimizing the sum of squared residuals between the observed values and the predicted values of \n",
    "the dependent variable. \n",
    "Ridge regression, on the other hand, adds a penalty term to the sum of squared residuals to shrink the estimated coefficients towards zero. \n",
    "This penalty term is proportional to the square of the magnitude of the coefficients, which means that larger coefficients are penalized more\n",
    "than smaller coefficients. The amount of shrinkage is controlled by a tuning parameter called lambda (λ).\n",
    "As λ increases, the amount of shrinkage increases, which means that the estimated coefficients become smaller and closer to zero. \n",
    "Ridge regression is useful when there is multicollinearity among the independent variables, which means that some independent variables are\n",
    "highly correlated with each other.\n",
    "In this case, OLS regression can produce unstable and unreliable estimates of the coefficients, while ridge regression can produce more stable\n",
    "and reliable estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699a12c5-33be-40b3-a50d-d540362b4e96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0896ec-1e16-4d3d-bc7b-11c6b86daebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "18. What is heteroscedasticity in regression and how does it affect the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab22e536-7e42-4d6b-b7ef-9b02e441a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "In regression analysis, heteroscedasticity refers to the unequal scatter of residuals or error terms. Specifically, it refers to the case \n",
    "where there is a systematic change in the spread of the residuals over the range of measured values.\n",
    "Heteroscedasticity is a problem because ordinary least squares (OLS) regression assumes that the residuals come from a population that has \n",
    "homoscedasticity, which means constant variance. When heteroscedasticity is present in a regression analysis, the results of the analysis\n",
    "become hard to trust.\n",
    "Specifically, heteroscedasticity increases the variance of the regression coefficient estimates, but the regression model doesn’t pick up on\n",
    "this. This makes it much more likely for a regression model to declare that a term in the model is statistically significant, when in fact it\n",
    "is not1. The simplest way to detect heteroscedasticity is with a fitted value vs. residual plot.\n",
    "Once you fit a regression line to a set of data, you can then create a scatterplot that shows the fitted values of the model vs. the residuals\n",
    "of those fitted values. The scatterplot shows how much variation in the dependent variable is explained by the independent variable(s).\n",
    "If there is heteroscedasticity, you will see a pattern in which the residuals are more spread out for some values of the independent variable\n",
    "than for others.\n",
    "There are several ways to fix heteroscedasticity in regression analysis such as transforming the dependent variable, redefining the dependent\n",
    "variable, or using weighted least squares regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6123ea-5b49-463f-b6d6-dc2e273e0100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001b97ec-9d54-421b-84a1-8e94377b9372",
   "metadata": {},
   "outputs": [],
   "source": [
    "19. How do you handle multicollinearity in regression analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98884408-4c0f-4527-955d-abcf3be1f08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multicollinearity in regression is a condition that occurs when some predictor variables in the model are correlated with other predictor \n",
    "variables. This means that as one predictor variable changes value, the other predictor variables also change in a specific direction.\n",
    "Multicollinearity can lead to skewed or misleading results, such as increased variance of the regression coefficients, wider confidence\n",
    "intervals, and less reliable probabilities. There are several ways to handle multicollinearity in regression analysis such as:\n",
    "    \n",
    "- Remove some of the highly correlated independent variables.\n",
    "- Linearly combine the independent variables, such as adding them together.\n",
    "- Partial least squares regression uses principal component analysis to create a set of uncorrelated components to replace the original\n",
    "  predictors.\n",
    "    \n",
    "It is important to note that there is no one-size-fits-all solution for handling multicollinearity in regression analysis.\n",
    "The best approach depends on the nature of the data and the research question being addressed.\n",
    "It is always a good idea to consult with a statistician or data analyst before deciding on an approach for handling multicollinearity in your\n",
    "data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15812193-c67e-4091-b2bb-959111e0f8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9d1991-cca4-4f3b-b93f-06c7f6a48e98",
   "metadata": {},
   "source": [
    "### Loss function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5360181d-4f0f-4667-8355-a2659dd65016",
   "metadata": {},
   "outputs": [],
   "source": [
    "21. What is a loss function and what is its purpose in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ee85d7-9570-4257-879f-cf183d76651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A loss function is a mathematical function that measures the difference between the predicted output and the actual output in a machine\n",
    "learning model.\n",
    "The purpose of a loss function is to provide guidance to the optimization algorithm to minimize the difference between the predicted output \n",
    "and the actual output. The optimization algorithm uses this guidance to adjust the model’s parameters to improve its performance. \n",
    "There are many types of loss functions, such as mean squared error (MSE), mean absolute error (MAE), log loss (cross-entropy loss),\n",
    "and Huber loss, among others. The choice of a loss function depends on the problem being solved and the nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c40e32-1c9e-483b-a89e-204d88e36816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaad6a7-11c0-492b-8291-fcd83d38fd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "22. What is the difference between a convex and non-convex loss function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e214d-5c7c-468f-b9dd-c6679286e97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "A convex loss function is a function whose second derivative is always non-negative, while a non-convex loss function is a function whose \n",
    "second derivative can be negative in some regions. \n",
    "Convex loss functions have only one minimum, which makes them easier to optimize, while non-convex loss functions can have multiple minima,\n",
    "which makes them harder to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567227b2-adc2-43df-b938-bfa6d9ae9ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200c178c-2e69-4481-b259-735437d262f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "23. What is mean squared error (MSE) and how is it calculated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a002f963-c59c-4cdc-b048-9072077df3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean squared error (MSE) is a common loss function used in regression problems that measures the average squared difference between the\n",
    "predicted output and the actual output. \n",
    "It is calculated by taking the average of the squared differences between each predicted value and its corresponding actual value.\n",
    "MSE is sensitive to outliers and can be affected by large errors in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600fd619-3cb5-4772-82ed-660b0cbb3cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325746c3-14c5-44a6-b7be-c8322cd6cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "24. What is mean absolute error (MAE) and how is it calculated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15da983-b73b-437a-ac29-219ac069f8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean absolute error (MAE) is another common loss function used in regression problems that measures the average absolute difference between \n",
    "the predicted output and the actual output.\n",
    "It is calculated by taking the average of the absolute differences between each predicted value and its corresponding actual value.\n",
    "MAE is less sensitive to outliers than MSE and can be more robust in cases where there are many outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2be256f-8325-41f0-b46f-89a1f803f37e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d855fb3f-6aba-4c0a-a40d-c981c6f87978",
   "metadata": {},
   "outputs": [],
   "source": [
    "25. What is log loss (cross-entropy loss) and how is it calculated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cdab0b-fbf8-4b78-8337-4e6e22f9f1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Log loss (cross-entropy loss) is a common loss function used in classification problems that measures how well a model predicts probabilities\n",
    "for each class label.\n",
    "It is calculated by taking the negative logarithm of the predicted probability for each correct class label and summing them up over all \n",
    "examples in the dataset.\n",
    "Log loss penalizes models more heavily for incorrect predictions with high confidence than for incorrect predictions with low confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b78ab1-d367-453d-b98d-d8a25ac8df6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb74bd5-bbac-4d07-8f57-b01870236e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "26. How do you choose the appropriate loss function for a given problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dacaad-f840-475c-b38a-813f7c804780",
   "metadata": {},
   "outputs": [],
   "source": [
    "The appropriate choice of a loss function depends on several factors such as the nature of the data, problem being solved, and desired\n",
    "properties of the model’s predictions.\n",
    "For example, if we want our model to be robust to outliers, we might choose MAE over MSE as our loss function.\n",
    "If we want our model to predict probabilities for each class label, we might choose log loss as our loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef716843-7e11-4579-aefb-be5b6f413669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfbf5cb-6cf2-40c4-9366-c1e182a8f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "27. Explain the concept of regularization in the context of loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e7a35b-2e3d-48ea-abd7-9295ad224645",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the model’s loss function that\n",
    "discourages it from fitting too closely to the training data.\n",
    "The penalty term can take different forms such as L1 regularization or L2 regularization, depending on how we want to constrain our model’s\n",
    "parameters during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d981458-b115-4862-8d63-5196b4202721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db82925-dd86-47c9-9d6e-7525954efc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "28. What is Huber loss and how does it handle outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a566fef0-ef33-4326-9f22-0caba22af89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Huber loss is a type of loss function used in regression problems that combines properties of both MSE and MAE by being less sensitive to \n",
    "outliers than MSE but more sensitive than MAE.\n",
    "It is defined piecewise as quadratic for small values of residuals and linear for large values of residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acdc92f-5c23-441b-8adc-cf142e9999d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f49c82-7f85-4f32-a89d-34c320207ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "29. What is quantile loss and when is it used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6463ba2d-a2de-4b77-9409-222b47182fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Quantile loss is another type of loss function used in regression problems that measures how well a model predicts quantiles of the target\n",
    "variable distribution instead of just its mean or median value.\n",
    "It penalizes under-predictions and over-predictions differently based on their quantile level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4a8904-b86e-441a-8162-48c3f844f563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12756800-c26b-4839-a624-ed1bf3718b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "30. What is the difference between squared loss and absolute loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5aba10-7390-4519-8a61-b036a631bf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "Squared loss and absolute loss are two different ways to measure the difference between predicted and actual values in regression problems. \n",
    "Squared loss measures this difference using squared values while absolute loss uses absolute values instead.\n",
    "Squared loss tends to be more sensitive to outliers than absolute loss because it squares errors before summing them up, while absolute loss \n",
    "treats all errors equally regardless of their magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b71ca66-3122-4d2b-a6f4-042aa431e8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ea1eb3c-9df9-4c9e-b567-5b3c8d309858",
   "metadata": {},
   "source": [
    "### Optimizer (GD):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3617c2-64b8-45f7-8d6c-e32d16a36c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "31. What is an optimizer and what is its purpose in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7006bdf5-688f-4a87-813a-a5fddc16c7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "In machine learning, an optimizer is an algorithm or method used to adjust the parameters of a model to minimize the error between the\n",
    "predicted output and the actual output. \n",
    "The purpose of an optimizer is to find the optimal set of parameters that minimize the cost function or loss function of a model. \n",
    "There are many types of optimizers used in machine learning, such as Gradient Descent (GD), Stochastic Gradient Descent (SGD), Adam, Adagrad,\n",
    "and RMSprop, among others. The choice of optimizer depends on the problem being solved and the nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cfdbeb-0efe-445c-8d49-bcfd6f75cc60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c885a3-f1ff-4061-a653-ab17740de0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "32. What is Gradient Descent (GD) and how does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae4025d-6b4b-45e2-8f3f-39c7f028d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Descent (GD) is an iterative optimization algorithm used to minimize a cost function by adjusting the parameters of a model. \n",
    "The idea behind GD is to take small steps in the direction of the negative gradient of the cost function at each iteration until we reach a\n",
    "minimum.\n",
    "The negative gradient points in the direction of steepest descent, which means that we are moving towards a minimum of the cost function.\n",
    "GD can be used for both linear and logistic regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e76adee-6a06-43ea-9ce2-7746704ca214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19282a5c-b194-40d8-8374-e98a56b82f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "33. What are the different variations of Gradient Descent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf22ad1a-5b99-4755-90b8-adc8c5f96903",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are three variations of Gradient Descent:\n",
    "    \n",
    "- Batch Gradient Descent: This method computes the gradient of the cost function over the entire training dataset at each iteration.\n",
    "- Stochastic Gradient Descent: This method computes the gradient of the cost function for each training example at each iteration.\n",
    "- Mini-batch Gradient Descent: This method computes the gradient of the cost function for a small random subset of training examples at each\n",
    "  iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da401484-1f42-44a2-9165-0946594d4c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc16a2c5-6bcd-4c58-8e4e-a4a0c78766f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "34. What is the learning rate in GD and how do you choose an appropriate value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29f3a75-98db-4cf6-861c-b875fc9bece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "The learning rate in GD is a hyperparameter that controls how much we adjust our parameters with respect to the gradient of the cost function.\n",
    "It determines how big our steps are during optimization. If we set our learning rate too high, we might overshoot our minimum and diverge from\n",
    "it. If we set our learning rate too low, we might converge too slowly or get stuck in local minima.\n",
    "Choosing an appropriate value for the learning rate is important for successful optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014adfd0-9e31-4e7f-aa80-ae3e2dd1274b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f744b68b-bbc1-4fa5-b97c-0d637febf849",
   "metadata": {},
   "outputs": [],
   "source": [
    "35. How does GD handle local optima in optimization problems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043fccb4-96e9-4917-bea9-e59833ed1de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GD can handle local optima in optimization problems by using random initialization and multiple restarts. \n",
    "By randomly initializing our parameters and running GD multiple times, we can increase our chances of finding a global minimum instead of \n",
    "getting stuck in a local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf71d51f-1421-4c39-aa55-9f44f9b26c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6e0ee8-da05-4e64-8390-b860fe133c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "36. What is Stochastic Gradient Descent (SGD) and how does it differ from GD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40556a3-65fc-4af4-b0e9-a193b361effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stochastic Gradient Descent (SGD) is a variation of GD that computes the gradient of the cost function for each training example at each\n",
    "iteration instead of computing it over the entire training dataset like Batch GD does.\n",
    "SGD introduces randomness into optimization, which can help it escape from local minima and converge faster than Batch GD when dealing with\n",
    "large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a82ee9-3675-44c7-8923-72f6c797ff93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27344196-73e7-4d3f-b6fb-6f81ee92deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "37. Explain the concept of batch size in GD and its impact on training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872d820e-ea07-4889-b0f2-9ca09fb5d4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "In Batch GD, batch size refers to how many training examples we use to compute our gradient at each iteration. \n",
    "A larger batch size means that we are using more data to compute our gradient, which can lead to more accurate gradients but also slower\n",
    "computation time per iteration. \n",
    "A smaller batch size means that we are using less data to compute our gradient, which can lead to noisier gradients but faster computation\n",
    "time per iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfa0ed6-fe0c-4c5b-bc84-aa9b09a5e0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d502b9e6-f7b1-4426-b4d8-ddc63af3881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "38. What is the role of momentum in optimization algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c1202e-6633-466f-9e0a-00f986f30fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Momentum is a technique used in optimization algorithms to speed up convergence by adding a fraction of the previous update vector to the \n",
    "current update vector during optimization. \n",
    "This helps smooth out oscillations in optimization and helps it converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95fcced-c4d4-4008-b6a2-fe2919c8a0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e933d7-5476-4f5d-bdb8-c1edaa314757",
   "metadata": {},
   "outputs": [],
   "source": [
    "39. What is the difference between batch GD, mini-batch GD, and SGD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ffc91-fb06-44b7-a0f9-3eb848093060",
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch GD computes gradients over all training examples at each iteration, while Mini-batch GD computes gradients over a small random subset of\n",
    "training examples at each iteration. SGD computes gradients over one training example at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3a3c0c-b1f7-4ecd-9cd3-578a63f9de33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88502272-6fe5-4c3f-9557-70f8c51af3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "40. How does the learning rate affect the convergence of GD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6c5d25-b3f3-4172-8679-f87557568883",
   "metadata": {},
   "outputs": [],
   "source": [
    "The learning rate affects convergence in GD by controlling how much we adjust our parameters with respect to our gradient at each iteration. \n",
    "If we set our learning rate too high, we might overshoot our minimum and diverge from it.\n",
    "If we set our learning rate too low, we might converge too slowly or get stuck in local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d64bcac-d879-40b5-a1d3-947a0bb9ba0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c66074c-d145-4a2f-bcbf-f119b4dff29e",
   "metadata": {},
   "source": [
    "### Regularization:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd10ee2e-2022-4c92-8c77-28dd36ba5c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "41. What is regularization and why is it used in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e50c0b-3c71-4210-8cfa-8fa604b722ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the loss function of a model. \n",
    "The penalty term discourages the model from fitting too closely to the training data and helps it generalize better to new, unseen data.\n",
    "Regularization can be applied to many types of models, such as linear regression, logistic regression, and neural networks.\n",
    "There are many types of regularization techniques, such as L1 and L2 regularization, ridge regression, elastic net regularization, \n",
    "and dropout regularization, among others. \n",
    "The choice of a regularization technique depends on the problem being solved and the nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c8104e-76dc-4f57-947b-968a3d04aa7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa173aab-80c9-47c1-b8a2-c34932651778",
   "metadata": {},
   "outputs": [],
   "source": [
    "42. What is the difference between L1 and L2 regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0731f66-acb3-433b-a215-58a87da600cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 and L2 regularization are two common types of regularization techniques used in machine learning.\n",
    "L1 regularization adds a penalty term that is proportional to the absolute value of the coefficients, while L2 regularization adds a penalty\n",
    "term that is proportional to the square of the coefficients.\n",
    "L1 regularization tends to shrink coefficients to zero, making it useful for feature selection, while L2 regularization tends to shrink\n",
    "coefficients evenly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d915b0-1bf7-423d-856b-20a53ff01ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48163b6b-f129-49c7-8a98-88bf29ba31a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "43. Explain the concept of ridge regression and its role in regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426513ea-5b65-473a-884a-aa5b6e47c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge regression is a type of linear regression that adds an L2 penalty term to the loss function of the model.\n",
    "The L2 penalty term discourages large weights in the model and helps prevent overfitting by shrinking the coefficients towards zero.\n",
    "Ridge regression can be used for both linear and logistic regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6b2b6a-4cde-4b00-9ba6-e37b40563b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9f7258-05c5-4799-83b0-e796b959c651",
   "metadata": {},
   "outputs": [],
   "source": [
    "44. What is the elastic net regularization and how does it combine L1 and L2 penalties?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1787133f-cbbc-4491-8366-c5a2f85b2dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Elastic net regularization is a type of regularization that combines both L1 and L2 penalties in a linear regression model. \n",
    "The elastic net method adds a quadratic part to the penalty term that is proportional to the square of the coefficients (L2) and an absolute\n",
    "value part that is proportional to the absolute value of the coefficients (L1). \n",
    "Elastic net regularization can be used for feature selection and helps prevent overfitting by shrinking coefficients towards zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7458ca9b-c125-4e01-8736-4a90b07c3358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bcd9ad-a0c1-44de-a744-ab9ff571aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "45. How does regularization help prevent overfitting in machine learning models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c69262-283f-498f-a30e-43e9d99d8965",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization helps prevent overfitting in machine learning models by adding a penalty term to the loss function that discourages large\n",
    "weights in the model. \n",
    "Large weights can cause overfitting by fitting too closely to the training data and not generalizing well to new data. \n",
    "Regularization helps models generalize better by shrinking weights towards zero and reducing their complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baddd199-602b-4d96-803c-59656ea5490d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0711d7a-ede1-4430-a1c6-77eaf832ca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "46. What is early stopping and how does it relate to regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a9ee2c-4854-4236-bde7-f7593e62fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Early stopping is a technique used in machine learning to prevent overfitting by stopping training when performance on a validation set stops\n",
    "improving or starts getting worse. \n",
    "Early stopping can be seen as a form of regularization because it prevents models from fitting too closely to the training data by stopping\n",
    "them before they start overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b78dfa-1e5f-4c07-b1f0-c1799aaf1f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e377f1d-b5bc-4c69-9347-c851369181ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "47. Explain the concept of dropout regularization in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f282b5a4-3126-45f3-bd04-9d6dd269d900",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dropout regularization is a technique used in neural networks to prevent overfitting by randomly dropping out nodes during training.\n",
    "During training, some number of layer outputs are randomly ignored or “dropped out.” This makes the layer appear as if it has a different\n",
    "number of nodes and connectedness than it actually does, which helps prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85563c9-7515-4580-91c9-6ce944dc3cad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c36172-a1bb-4713-b67b-4fed338b6170",
   "metadata": {},
   "outputs": [],
   "source": [
    "48. How do you choose the regularization parameter in a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94ef45c-11d0-4945-833d-645a12afe30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "The choice of a regularization parameter depends on several factors such as the nature of the data, problem being solved, \n",
    "and desired properties of the model’s predictions. \n",
    "One common way to choose a regularization parameter is through cross-validation, which involves splitting the data into training and\n",
    "validation sets and testing different values of the parameter on each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74bfe78-b195-45db-a593-b610b223cd4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482250f9-ee28-4e22-8693-fad481cea8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "49. What is the difference between feature selection and regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dff736e-6068-496c-8796-05e3948f8e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature selection and regularization are two different techniques used in machine learning for reducing overfitting in models. \n",
    "Feature selection involves selecting only relevant features from a dataset while ignoring irrelevant ones, while regularization involves \n",
    "adding a penalty term to the loss function that discourages large weights in models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82706c2c-fc46-47f7-b1ec-0c8d6e2f2444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151ec08e-8544-443f-9ed5-064112b292e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "50. What is the trade-off between bias and variance in regularized models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1e183c-437d-45b9-850c-9c55020f7a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "The trade-off between bias and variance in regularized models depends on several factors such as the nature of the data, problem being solved,\n",
    "and desired properties of the model’s predictions.\n",
    "Regularized models tend to have higher bias but lower variance than unregularized models because they are less complex and have smaller\n",
    "weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4937557-0bc8-459b-bb8a-74230ad20529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe2bc13f-0ec2-448e-a8ee-66e7b71c3f6e",
   "metadata": {},
   "source": [
    "### SVM:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfbcebd-5278-433d-a7b0-2f424964f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "51. What is Support Vector Machines (SVM) and how does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4f39fd-3722-465e-9a1c-a69ea7336f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Support Vector Machines (SVM) is a powerful machine learning algorithm used for classification and regression tasks. \n",
    "It is a supervised learning algorithm that analyzes data for classification and regression analysis.\n",
    "SVM finds a hyperplane that segregates the labeled dataset into two classes. The hyperplane is defined as f(X) = w^T * X + b, where w is the\n",
    "weight vector that you want to minimize, X is the data that you’re trying to classify, and b is the linear coefficient estimated from the \n",
    "training data. This equation defines the decision boundary that the SVM returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ac5eaa-8cfd-4cda-a639-4682889cee76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b9b4b-64b6-4cbc-84d0-aae726a56384",
   "metadata": {},
   "outputs": [],
   "source": [
    "52. How does the kernel trick work in SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e1d49f-b50a-4d16-b42a-bace2a65b13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "The kernel trick in SVM is a method of using a linear classifier to solve non-linear problems by mapping the original input space into a \n",
    "higher-dimensional space. \n",
    "The kernel function computes the inner product between two points in a transformed feature space, which allows us to compute the decision \n",
    "boundary in this higher-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ea8d20-8ae3-42d4-816d-ead13af2cfce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d37b68-18f3-4341-be66-0ca5423ba14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "53. What are support vectors in SVM and why are they important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8235423b-49ab-48ea-902a-207cf2462eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Support vectors are data points that lie closest to the decision boundary, and they are important because they determine the position of the\n",
    "decision boundary. The margin in SVM is defined as the distance between the decision boundary and the closest data points from each class.\n",
    "The larger the margin, the better generalization performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1081a66a-a05f-4ed6-a58d-cd0548297d10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2512f6-46de-4a18-a923-810cdb4f50af",
   "metadata": {},
   "outputs": [],
   "source": [
    "54. Explain the concept of the margin in SVM and its impact on model performanc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f874d1d-beec-40b5-8a53-b5af0346bfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "The margin in SVM is the distance between the decision boundary and the closest data points from each class. \n",
    "The larger the margin, the better generalization performance of the model. In other words, a larger margin means that the model is more\n",
    "robust to noise and outliers in the data. \n",
    "The margin is important because it determines the position of the decision boundary and how well the model can generalize to new, unseen data. \n",
    "SVM tries to maximize this margin by finding the hyperplane with maximum margin, which is called the optimal hyperplane. \n",
    "The optimal hyperplane is the one that separates the two classes with maximum margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b54037f-e2ad-44a6-8199-aa71a9ffb249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753e6466-d916-49bb-a0e5-cc152c81c048",
   "metadata": {},
   "outputs": [],
   "source": [
    "55. How do you handle unbalanced datasets in SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06566dda-4e6b-471f-b5e5-437957907256",
   "metadata": {},
   "outputs": [],
   "source": [
    "In an unbalanced dataset, SVM permits a soft margin technique by introducing slack variables to allow certain constraints to be violated.\n",
    "This allows for misclassifications to happen, which can improve generality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac828ef8-688b-4e96-a0e7-da40f2011b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b80278-b609-4235-abc8-a7d1002dbd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "56. What is the difference between linear SVM and non-linear SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac0dbf3-65bd-44ee-a3f4-3a4330e33769",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear SVM works on linearly separable data by using a straight line to create a wide margin between different classes.\n",
    "Non-linear SVM uses kernel functions to transform non-linear spaces into linear spaces so that data can be classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8d2a6e-3c44-4766-b69d-6894eacc10b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef252e0f-ffe9-4fef-bb41-434368712c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "57. What is the role of C-parameter in SVM and how does it affect the decision boundary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784ac95a-a84e-4498-b933-1145beb107f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "The C parameter in SVM controls how much you want to punish your model for each misclassified point for a given curve.\n",
    "A smaller value of C allows for a larger margin, potentially leading to more misclassifications on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d897425-06de-4b08-8ee1-e219e806e573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e78f68-b296-4bae-9518-ee52e74c5dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "58. Explain the concept of slack variables in SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2544631-a4c6-4980-9b03-de14ef4d632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Slack variables are introduced in SVM to allow certain constraints to be violated, allowing certain training points to be within the margin.\n",
    "The importance of support vectors lies in their ability to determine the position of the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d5c898-187c-4d38-8d87-ee99d4ca7f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4855c397-ac36-492f-ad66-ffbd3de16beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "59. What is the difference between hard margin and soft margin in SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9647b0db-82bd-40ef-ad2f-3f7eae4e38c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "The difference between hard margin and soft margin in SVM lies in the separability of data. If our data is linearly separable, we go for a\n",
    "hard margin.However, if this is not the case, it won’t be feasible to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a08d069-db96-4caa-a3d3-f07cc8c2cc01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760ffbdb-1fa5-4775-81e0-f2650fe30470",
   "metadata": {},
   "outputs": [],
   "source": [
    "60. How do you interpret the coefficients in an SVM model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e13a45-6506-4674-8484-527e2211770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "In an SVM model, the coefficients represent the weights assigned to each feature in order to make predictions. \n",
    "The weights represent the vector coordinates which are orthogonal to the hyperplane and their direction indicates the predicted class. \n",
    "The absolute size of the coefficients in relation to each other can then be used to determine feature importance for the data separation task.\n",
    "For example, if only one feature is used for separation, the hyperplane would be orthogonal to that axis.\n",
    "So, you could say that the absolute size of the coefficient relative to the other ones gives an indication of how important the feature was \n",
    "for the separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f51c36-6489-4018-a783-e9bbc0d4c46f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f1337d1-c25a-4a13-9229-de924a7590c3",
   "metadata": {},
   "source": [
    "### Decision Trees:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1350b4-5d51-49ac-a0ee-7d6f62fea442",
   "metadata": {},
   "outputs": [],
   "source": [
    "61. What is a decision tree and how does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdf8fd5-63d4-45f2-aff9-3f417998fa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A decision tree is a type of supervised learning algorithm used for classification and regression tasks.\n",
    "It works by recursively splitting the data into subsets based on the values of one or more input features until each subset is homogeneous \n",
    "with respect to the target variable.\n",
    "The result is a tree-like structure where each internal node represents a test on an input feature, each branch represents the outcome of the\n",
    "test, and each leaf node represents a class label or a numerical value.\n",
    "Decision trees are easy to interpret and can handle both categorical and numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ca6ff9-39dc-4aa5-93cc-a0cf113ba54b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077f82e6-c2fe-4279-b75b-aaf3dc5a3bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "62. How do you make splits in a decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a4a235-f922-49a3-864a-f2cc23d7b89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "In a decision tree, splits are made by selecting the feature that best separates the data into homogeneous subsets with respect to the target\n",
    "variable. The goal is to maximize the information gain or minimize the impurity measure at each split.\n",
    "The feature with the highest information gain or lowest impurity measure is selected as the splitting criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd62205-4d8e-4b21-b834-fef598910a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41049b9a-0b3f-4dd6-9326-774cd365d1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "63. What are impurity measures (e.g., Gini index, entropy) and how are they used in decision trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4094cf-4601-4d41-9cb0-8cbe7bee60e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Impurity measures are used in decision trees to quantify how well a split separates the data into homogeneous subsets with respect to the \n",
    "target variable. Two common impurity measures are Gini index and entropy. \n",
    "Gini index measures how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to th\n",
    "e distribution of labels in the subset, while entropy measures how much uncertainty there is in a set of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24f9e2f-c3c9-47fe-82eb-9b72017fe59a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbf6903-44ab-4c75-98d7-5d4ce2a6a483",
   "metadata": {},
   "outputs": [],
   "source": [
    "64. Explain the concept of information gain in decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762d6aaa-4dec-4e1c-b01f-c61fa6db4943",
   "metadata": {},
   "outputs": [],
   "source": [
    "Information gain is a measure used in decision trees to quantify how much information is gained by splitting the data on a particular feature.\n",
    "It is calculated as the difference between the impurity measure before and after the split. \n",
    "The feature with the highest information gain is selected as the splitting criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d65b0-895a-47fb-b378-ec75753c85d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb5a803-f2ba-44b0-9606-b632c7195028",
   "metadata": {},
   "outputs": [],
   "source": [
    "65. How do you handle missing values in decision trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcaa85e-4a04-4264-85a5-ccbbe60936b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Missing values in decision trees can be handled by either ignoring them, imputing them with mean or median values, or using surrogate splits\n",
    "that allow missing values to be assigned to one of the branches based on other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb39082-02de-4f96-9f35-31a7ade94c98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb2e903-a772-4bc7-8bd4-b5970b2c4f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "66. What is pruning in decision trees and why is it important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f88028-efd6-461c-815f-b955222dcf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pruning in decision trees is a technique used to prevent overfitting by removing branches that do not improve generalization performance on\n",
    "validation data.\n",
    "Pruning can be done either pre-pruning, where branches are removed during training based on some stopping criterion, or post-pruning,\n",
    "where branches are removed after training based on some pruning criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a623c9-4c12-4c8a-af9d-fbafda5c8952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2083fb17-1f5a-4b88-ab1d-279ce6bb6f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "67. What is the difference between a classification tree and a regression tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e279bc50-cefc-4240-bd14-b66d7e36aec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "A classification tree is a type of decision tree used for classification tasks, while a regression tree is used for regression tasks. \n",
    "Classification trees predict categorical labels, while regression trees predict numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e516b799-a4e5-4534-aa10-3753ea54afa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dfcf2b-03b5-4396-9b36-1964c1b4735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "68. How do you interpret the decision boundaries in a decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6969492-43f6-4665-a067-138eaacd1842",
   "metadata": {},
   "outputs": [],
   "source": [
    "Decision boundaries in decision trees are represented by splits on input features that separate data into homogeneous subsets with respect to \n",
    "the target variable. \n",
    "Each split creates two new branches that represent different outcomes of the test on that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f468a19-1303-41d7-9411-fa430de5c6ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e1794d-c84f-45f4-b1fa-e31c73511234",
   "metadata": {},
   "outputs": [],
   "source": [
    "69. What is the role of feature importance in decision trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0651010-6629-4978-9aa9-7a6329c0ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature importance in decision trees refers to how much each input feature contributes to improving model performance by reducing impurity or\n",
    "increasing information gain at each split. \n",
    "Feature importance can be calculated by summing up how much each feature improves impurity or information gain across all splits in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b36c84e-0942-4158-86ec-69d6fe750d24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb47061-d6c7-4630-b906-3144b13c0297",
   "metadata": {},
   "outputs": [],
   "source": [
    "70. What are ensemble techniques and how are they related to decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1423c1-5f58-4372-81bf-524eb7f02163",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ensemble techniques are machine learning methods that combine multiple models to improve performance and reduce overfitting.\n",
    "Ensemble techniques related to decision trees include bagging, boosting, and random forests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a48a2a2-5e38-4674-b91e-d99bcdfd79b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b51b35c3-de4f-4a8c-aeb6-40596ae51333",
   "metadata": {},
   "source": [
    "### Ensemble Techniques:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8b5c59-0f95-45f3-b536-e61edd85cb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "71. What are ensemble techniques in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942025a-43cd-4143-a1cf-4d86ac0b7eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ensemble techniques are a set of machine learning methods that combine multiple models to improve the overall performance of the system.\n",
    "The idea behind ensemble techniques is to reduce the variance and bias in the predictions by combining multiple models that are trained on\n",
    "different subsets of the data or using different algorithms. \n",
    "There are three main types of ensemble techniques: bagging, boosting, and stacking. \n",
    "Bagging allows multiple similar models with high variance to be averaged to decrease variance. \n",
    "Boosting builds multiple incremental models to decrease bias while keeping variance small.\n",
    "Stacking is a different paradigm that explores a space of different models for the same problem. \n",
    "The idea is to attack a learning problem with different types of models that can learn some part of the problem but not the whole space of\n",
    "the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2515f2c9-88a1-4421-9ffc-71d0c7675732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565f11b5-5678-4d0d-900b-5a09b91311a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "72. What is bagging and how is it used in ensemble learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0e5b76-3e62-4e92-829e-89f09aa302fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bagging is an ensemble learning method that reduces variance and improves accuracy by averaging multiple similar models with high variance. \n",
    "It involves training each model on a different subset of the training data and then combining their predictions by averaging them. \n",
    "This method can increase bias and may not work well with noisy data and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623c6150-9739-4cda-aaff-941157f04892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b024955-0a68-4d2b-9cd6-3bc2b51547f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "73. Explain the concept of bootstrapping in bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15e3b8e-17ed-412a-9038-14da31155835",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bootstrapping is a sampling method used in bagging, where a sample is chosen out of a set using the replacement method.\n",
    "The learning algorithm is then run on the samples selected. The bootstrapping technique uses sampling with replacements to make the selection \n",
    "procedure completely random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302cf86f-c9e1-4f85-aa0e-4218b7ad0585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c520177-f566-4c1b-a151-adc6e51ad6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "74. What is boosting and how does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9344c7-6b60-4572-ab8b-934f6a0ffc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boosting is an ensemble learning method that improves accuracy and reduces bias by building multiple incremental models while keeping variance\n",
    "small. \n",
    "It involves adding ensemble members sequentially that correct the predictions made by prior models and outputs a weighted average of the \n",
    "predictions. However, it can overfit with noisy data and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd18b845-186c-43c3-943d-ac2834d77f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbc4095-a9de-452f-ad2c-56702a981410",
   "metadata": {},
   "outputs": [],
   "source": [
    "75. What is the difference between AdaBoost and Gradient Boosting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6bab79-848c-4080-85be-5c99efd8d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaBoost and Gradient Boosting are two popular boosting algorithms used in machine learning. \n",
    "AdaBoost builds multiple weak learners on different subsets of training data while Gradient Boosting builds multiple decision trees\n",
    "sequentially, where each tree corrects the errors made by the previous tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a30a1-5c21-476a-9d67-590346df84c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af41bf65-659a-48ba-8677-f71d03efd8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "76. What is the purpose of random forests in ensemble learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ddfbe-2e56-4892-b1ea-87af86e2936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random forests are an ensemble machine learning method used for classification, regression, and other tasks that operate by constructing a\n",
    "multitude of decision trees at training time.\n",
    "For classification tasks, the output of the random forest is the class selected by most trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c4ac2e-f675-4978-a887-87bfc5ba6193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46cfd0a-92b7-4687-b142-a52852ce3aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "77. How do random forests handle feature importance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb3d73-8513-40a6-b09e-b0e0370bc0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random forests handle feature importance by measuring how much each feature contributes to the prediction of the dependent variable using \n",
    "techniques such as average impurity decrease or average accuracy decrease across all decision trees in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc15d1b-f152-4003-9578-9d5fc6c0a8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cfb492-7657-4010-aad7-2bf2dd551840",
   "metadata": {},
   "outputs": [],
   "source": [
    "78. What is stacking in ensemble learning and how does it work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703b0da6-688a-490e-af8f-1a1b9055a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stacking is an ensemble learning method that improves prediction accuracy by combining multiple classifications or regression models. \n",
    "It explores a space of different models for the same problem and attacks a learning problem with different types of models that can learn \n",
    "some part of the problem but not the whole space of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbb25a3-5ed3-45cc-b15e-1d9fb1702a58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e95c773-991a-481c-83ad-0752e22b1a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "79. What are the advantages and disadvantages of ensemble techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b10cad-6629-4689-a59d-5a5ea62f8241",
   "metadata": {},
   "outputs": [],
   "source": [
    "The advantages of ensemble techniques include reducing variance and improving accuracy, improving prediction accuracy, reducing overfitting,\n",
    "handling missing data, and providing estimates for variable importance among others.\n",
    "The disadvantages include being costly in terms of time complexity, memory consumption, and processing power, being complex and time-consuming\n",
    "to implement, being difficult to interpret, among others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a963a0d1-446d-4b19-ae35-194b373061d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e033c2-59aa-4b97-b5ec-d7a2367274c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "80. How do you choose the optimal number of models in an ensemble?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d3c9e9-dedf-42a6-8ca9-1592eadc60eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are no restrictions or guidelines on the number of models in an ensemble; you can start with as few as three models and keep the number\n",
    "as a hyperparameter if training cost is less.\n",
    "Typically, you will observe a slanted ‘L’ shaped curve for MSE vs of models plot; you can take the elbow point as the final number of models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
